{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzoIc-JiXgG1"
   },
   "outputs": [],
   "source": [
    "# !pip install -U catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5379,
     "status": "ok",
     "timestamp": 1591607813584,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "13996855833058403790"
     },
     "user_tz": -540
    },
    "id": "gK8C0rrOT298",
    "outputId": "4e3b6c2d-2676-461d-95d8-f920eb1bfcfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling albumentations-0.4.5:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.6/dist-packages/albumentations-0.4.5.dist-info/*\n",
      "    /usr/local/lib/python3.6/dist-packages/albumentations/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled albumentations-0.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15846,
     "status": "ok",
     "timestamp": 1591607824056,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "13996855833058403790"
     },
     "user_tz": -540
    },
    "id": "l0-nS9z3T215",
    "outputId": "5bcf2466-c081-4ec8-d473-6741fd8879de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/albu/albumentations\n",
      "  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-efnclhma\n",
      "  Running command git clone -q https://github.com/albu/albumentations /tmp/pip-req-build-efnclhma\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: imgaug<0.2.7,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (0.2.6)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (3.13)\n",
      "Requirement already satisfied, skipping upgrade: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (4.1.2.30)\n",
      "Requirement already satisfied, skipping upgrade: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.16.2)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.1)\n",
      "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (7.0.0)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.7)\n",
      "Building wheels for collected packages: albumentations\n",
      "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for albumentations: filename=albumentations-0.4.5-cp36-none-any.whl size=65038 sha256=c4d9c8f21cdbbe35afda7f262d55b314a59d9eb19d9a72db4bfce2ffc3b40e97\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ma0zkqjs/wheels/45/8b/e4/2837bbcf517d00732b8e394f8646f22b8723ac00993230188b\n",
      "Successfully built albumentations\n",
      "Installing collected packages: albumentations\n",
      "Successfully installed albumentations-0.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -U git+https://github.com/albu/albumentations --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 989,
     "status": "error",
     "timestamp": 1591613530489,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "13996855833058403790"
     },
     "user_tz": -540
    },
    "id": "b_CzD2QbZAk2",
    "outputId": "240511a7-b80a-47a9-a40d-ce81a66a5167"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: unrecognized arguments: #追加\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random\n",
    "import cv2\n",
    "import glob\n",
    "import collections\n",
    "import time\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader,Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "import albumentations as albu\n",
    "from albumentations import pytorch as AT\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Jupyterでインライン表示するための宣言\n",
    "%matplotlib inline #追加\n",
    "# import catalyst\n",
    "# from catalyst.dl import SupervisedRunner\n",
    "# from catalyst.dl.callbacks import AccuracyCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Mn13vL8hRn5"
   },
   "outputs": [],
   "source": [
    "name_labels = {'EmperorPenguin': 1, 'RockhopperPenguin': 2, 'GalapagosPenguin': 3, 'KingPenguin':4,\n",
    "         'White-flipperedPenguin':5,'HumboldtPenguin': 6 ,'MagellanicPenguin': 7,'AfricanPenguin': 8,'LittlePenguin': 9,\n",
    "         'ChinstrapPenguin': 10,'FiordlandPenguin':11,'Erect-crestedPenguin':12,'SnaresIslandsPenguin':13,'RoyalPenguin':14,\n",
    "         'MacaroniPenguin':15,'AdeliePenguin':16,'GentooPenguin':17,'Yellow-eyedPenguin':18}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y8-q-gNsZHP3"
   },
   "outputs": [],
   "source": [
    "class PRDataset(Dataset):\n",
    "    def __init__(self, datatype: str = 'train', transforms=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transforms = transforms \n",
    "        self.datatype = datatype\n",
    "        path = './images'\n",
    "        # 拡張子がjpegのみ取得\n",
    "        if datatype == 'train':\n",
    "          file_paths = glob.glob('./images/*' + '/*.jpg')\n",
    "        elif datatype == 'valid':\n",
    "          file_paths = glob.glob('./val/*.jpg')\n",
    "        else:\n",
    "          file_paths = glob.glob('./test/*.jpg')\n",
    "        for path in file_paths:\n",
    "          self.image_paths.append(path)\n",
    "          if datatype != 'test':\n",
    "            label = path.split('/')[-1].split('_')[0]\n",
    "            self.labels.append(name_labels[label])\n",
    "\n",
    "        self.num_samples = len(self.image_paths)\n",
    "        print(f\"[Detection Dataset] size: {self.num_samples}\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        augmented = self.transforms(image=image)\n",
    "        image = augmented['image']\n",
    "        if self.datatype != 'test':\n",
    "          label = self.labels[index]\n",
    "          return image, label\n",
    "        else:\n",
    "          # Noneだとダメらしい\n",
    "          return image, \"None\"   \n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rv7xX-lC2YIs"
   },
   "outputs": [],
   "source": [
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "# train用のデータ拡張\n",
    "data_transforms = albu.Compose([\n",
    "    albu.Resize(img_height, img_width),\n",
    "    albu.HorizontalFlip(p=0.5),\n",
    "    albu.Normalize(),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# test用のデータ拡張\n",
    "data_transforms_test = albu.Compose([\n",
    "    albu.Resize(img_height, img_width),\n",
    "    albu.Normalize(),\n",
    "    ToTensorV2()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17176,
     "status": "ok",
     "timestamp": 1591607825406,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "13996855833058403790"
     },
     "user_tz": -540
    },
    "id": "78N1N_xmAORd",
    "outputId": "7cdd049d-6ec6-44d2-a25c-96c17f9a0781"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Detection Dataset] size: 2218\n",
      "[Detection Dataset] size: 89\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PRDataset(datatype='train', transforms=data_transforms)\n",
    "valid_dataset = PRDataset(datatype='valid', transforms=data_transforms_test)\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vn759fh-_8BV"
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, criterion, optimizer, device, grad_acc=1):\n",
    "    model.train()\n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = 0.\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels=torch.from_numpy(np.array(labels))\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        # Gradient accumulation\n",
    "        if (i % grad_acc) == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "    total_loss /= len(data_loader)\n",
    "    metrics = {'train_loss': total_loss}\n",
    "    return metrics\n",
    "def eval(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    num_correct = 0.\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            num_correct += torch.sum(preds == labels.data)\n",
    "        total_loss /= len(data_loader)\n",
    "        num_correct /= len(data_loader.dataset)\n",
    "        metrics = {'valid_loss': total_loss, 'val_acc': num_correct}\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PfwvPTOYdqcS"
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "model = models.resnext50_32x4d(pretrained=True)\n",
    "# model = models.resnext50_32x4d(\"output.pth\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "epochs = 5\n",
    "num_class = 18\n",
    "output_path = './output/catalyst'\n",
    "loaders = {\"train\": train_loader, \"valid\": valid_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 720424,
     "status": "ok",
     "timestamp": 1591610659135,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "13996855833058403790"
     },
     "user_tz": -540
    },
    "id": "VDArmWn9JC4D",
    "outputId": "7b0df21d-5417-4d0f-c5d6-8877953cfa1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train start !\n",
      "epoch 0 start !\n",
      "epoch: 0  {'train_loss': 0.7354396602937153} {'valid_loss': 2.068526824315389, 'val_acc': tensor(0.5281, device='cuda:0')}\n",
      "epoch 1 start !\n",
      "epoch: 1  {'train_loss': 0.5696406903011458} {'valid_loss': 2.4503137270609536, 'val_acc': tensor(0.5056, device='cuda:0')}\n",
      "epoch 2 start !\n",
      "epoch: 2  {'train_loss': 0.4333643302321434} {'valid_loss': 2.071386913458506, 'val_acc': tensor(0.5506, device='cuda:0')}\n",
      "epoch 3 start !\n",
      "epoch: 3  {'train_loss': 0.3016752179179873} {'valid_loss': 1.6000480651855469, 'val_acc': tensor(0.6180, device='cuda:0')}\n",
      "epoch 4 start !\n",
      "epoch: 4  {'train_loss': 0.33400224191801886} {'valid_loss': 2.222716808319092, 'val_acc': tensor(0.5506, device='cuda:0')}\n",
      "epoch 5 start !\n",
      "epoch: 5  {'train_loss': 0.1846762572016035} {'valid_loss': 1.8656799793243408, 'val_acc': tensor(0.5955, device='cuda:0')}\n",
      "epoch 6 start !\n",
      "epoch: 6  {'train_loss': 0.11048330409186227} {'valid_loss': 1.8652525742848713, 'val_acc': tensor(0.6966, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "epochs = 5\n",
    "num_class = 18\n",
    "\n",
    "epochs = 7\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model\n",
    "criterion = criterion\n",
    "# Model を multi-gpu したり、FP16 対応したりする\n",
    "model = model.to(device)\n",
    "print('Train start !')\n",
    "for epoch in range(epochs):\n",
    "    print(f'epoch {epoch} start !')\n",
    "    metrics_train = train(model, train_loader, criterion, optimizer, device)\n",
    "    metrics_eval = eval(model, valid_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    # Logger 周りの処理\n",
    "    # print するためのごちゃごちゃした処理\n",
    "    print(f'epoch: {epoch} ', metrics_train, metrics_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HT6XzZmRg_mo"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './output_7.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01Yl-aK0uvqi"
   },
   "source": [
    "## testデータで検証する！！！！！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1591613430146,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "13996855833058403790"
     },
     "user_tz": -540
    },
    "id": "7gUbyC45uOqy",
    "outputId": "eb0917f5-ca59-4e0e-ceb4-510a69ed2130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Detection Dataset] size: 98\n"
     ]
    }
   ],
   "source": [
    "test_dataset = PRDataset(datatype='test', transforms=data_transforms_test)\n",
    "num_workers = 0\n",
    "batch_size = 1\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHdAnLmGg1yl"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0vdPW_92u49M"
   },
   "outputs": [],
   "source": [
    "def predict(model, data_loader, device):\n",
    "    predict_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "          #貼り付け\n",
    "            inputs = inputs.to(device)\n",
    "            # labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predict_list.append(preds[0])\n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xf81Vk4awiyd"
   },
   "outputs": [],
   "source": [
    "label = predict(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "64-rV_0vlZGs"
   },
   "outputs": [],
   "source": [
    "def get_key_from_value(d, val):\n",
    "    keys = [k for k, v in d.items() if v == val]\n",
    "    if keys:\n",
    "        return keys[0]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1CVR0FM8AlwSHmKBVEVLSGi9Q2UQkGMPi"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38289,
     "status": "ok",
     "timestamp": 1591614034304,
     "user": {
      "displayName": "遠藤丈",
      "photoUrl": "",
      "userId": "13996855833058403790"
     },
     "user_tz": -540
    },
    "id": "WqNdYucgNgH3",
    "outputId": "1cab338c-fbfd-4904-c2c7-7a5c71261d2f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_paths = glob.glob('./test/*.jpg')\n",
    "for index, path in enumerate(test_paths):\n",
    "  im = Image.open(path)\n",
    "  #画像をarrayに変換\n",
    "  im_list = np.asarray(im)\n",
    "  plt.title(get_key_from_value(name_labels, label[index]))\n",
    "  #貼り付け\n",
    "  plt.imshow(im_list)\n",
    "  #表示\n",
    "  plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jrA0EJHYlInh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPK5sPVog2BP4wdLqUG53kH",
   "mount_file_id": "1PSMwmm46FlQJqpdpZH3ET1rk7oz22yVF",
   "name": "P_classify.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
